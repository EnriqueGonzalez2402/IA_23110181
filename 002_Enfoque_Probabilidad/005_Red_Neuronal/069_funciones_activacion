# Comparaci贸n visual de funciones de activaci贸n
# ----------------------------------------------------------
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(-5,5,400)

# Diferentes activaciones
step = np.where(x>=0,1,0)
sigmoid = 1 / (1 + np.exp(-x))
tanh = np.tanh(x)
relu = np.maximum(0,x)
leaky_relu = np.where(x>0, x, 0.1*x)

plt.figure(figsize=(10,6))
plt.plot(x, step, label='Escal贸n')
plt.plot(x, sigmoid, label='Sigmoide')
plt.plot(x, tanh, label='tanh')
plt.plot(x, relu, label='ReLU')
plt.plot(x, leaky_relu, label='Leaky ReLU')
plt.legend()
plt.title("Funciones de activaci贸n")
plt.xlabel("x"); plt.ylabel("f(x)")
plt.grid(True)
plt.show()
